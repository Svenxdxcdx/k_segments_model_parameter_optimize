{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586f2df8-eae9-4724-b2f2-f7d9dba0be2e",
   "metadata": {},
   "source": [
    "# Predicting Dynamic Memory Requirements for Scientific Workflow Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129e5ec-9ac8-4cd9-b0c6-2cb3819ab27f",
   "metadata": {},
   "source": [
    "### Setup BASE_DIR\n",
    "- Path to tsb_resource_allocation_data\n",
    "- No / at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb755db0-ab45-4245-b39a-fbb7904cc659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'C:/privat/Bachelor_Work/pytonProject/k-segments-traces-main/k-segments-traces-main' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eddd0511-7e8f-4f7b-beb5-c57cbbc0fe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from tsb_resource_allocation.witt_task_model import WittTaskModel\n",
    "from tsb_resource_allocation.tovar_task_model import TovarTaskModel\n",
    "from tsb_resource_allocation.simulation import Simulation\n",
    "from tsb_resource_allocation.k_segments_model import KSegmentsModel\n",
    "from tsb_resource_allocation.file_events_model import FileEventsModel\n",
    "from tsb_resource_allocation.default_model import DefaultModel\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "# Helper methods\n",
    "\n",
    "def get_file_names(directory, number_of_files = -1):\n",
    "    file_names = [name.rsplit('_',1)[0] for name in os.listdir(directory) if not os.path.isdir(f\"{directory}{name}\") and name.endswith(\"_memory.csv\")]\n",
    "    if number_of_files != -1:\n",
    "        return file_names[:number_of_files]\n",
    "    return file_names\n",
    "\n",
    "def run_simulation(directory, training, test, monotonically_increasing = True, k = 4, collection_interval = 2):\n",
    "    \n",
    "    # MODELS\n",
    "    simulations = []\n",
    "    \n",
    "    # KSegments retry: selective\n",
    "    task_model = KSegmentsModel(k = k, monotonically_increasing = monotonically_increasing)\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'selective', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "    \n",
    "    # KSegments retry: selective - NO UNDERPREDICTION\n",
    "    task_model = KSegmentsModel(k = k, monotonically_increasing = monotonically_increasing, time_mode = -1)\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'selective', provided_file_names = training)\n",
    "    #simulations.append(simulation)\n",
    "    \n",
    "    # KSegments retry: partial\n",
    "    task_model = KSegmentsModel(k = k, monotonically_increasing = monotonically_increasing)\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'partial', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "    \n",
    "    # WITT LR MEAN+- TASK MODEL \n",
    "    task_model = WittTaskModel(mode = \"mean+-\")\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'full', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "\n",
    "    # TOVAR TASK MODEL - full retry\n",
    "    task_model = TovarTaskModel()\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'full', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "    \n",
    "     # TOVAR TASK MODEL - tovar retry\n",
    "    task_model = TovarTaskModel()\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'tovar', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "    \n",
    "    # Default Model\n",
    "    task_model = DefaultModel()\n",
    "    simulation = Simulation(task_model, directory, retry_mode = 'full', provided_file_names = training)\n",
    "    simulations.append(simulation)\n",
    "    \n",
    "    waste, retries, runtimes = [0 for _ in range(len(simulations))],[0 for _ in range(len(simulations))],[0 for _ in range(len(simulations))]\n",
    "    for file_name in test:\n",
    "        for i,s in enumerate(simulations):\n",
    "            result = s.execute(file_name, True)\n",
    "            waste[i] += ((result[0]/1000) * collection_interval)\n",
    "            retries[i] += result[1]\n",
    "            runtimes[i] += (result[2] * collection_interval)\n",
    "    \n",
    "    avg_waste = list(map(lambda w: w / len(test), waste))\n",
    "    avg_retries = list(map(lambda r: r / len(test), retries))\n",
    "    avg_runtime = list(map(lambda r: r / len(test), runtimes))\n",
    "    \n",
    "    return avg_waste, avg_retries, avg_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed997b3-5e2f-4ad2-b4d1-5c958e2bf430",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14677a0e-6819-41ef-aeed-530c2403aa5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# OUTPUT = ( [Waste: [Witt: 25, Tovar: 25, k-segments:25], [50] , [75]], [Retries], [Runtime])\n",
    "def benchmark_task(task_dir = f'{BASE_DIR}/eager/markduplicates'):\n",
    "    directory = task_dir\n",
    "    file_names_orig = get_file_names(directory)\n",
    "\n",
    "    percentages = [0.25, 0.5, 0.75]\n",
    "\n",
    "    x = []\n",
    "    y_waste = []\n",
    "    y_retries = []\n",
    "    y_runtime = []\n",
    "\n",
    "    file_names = list(filter(lambda x: len(pd.read_csv(f'{directory}/{x}_memory.csv', skiprows = 3)) >= 8, file_names_orig))\n",
    "    if len(file_names) == 0:\n",
    "        return -1\n",
    "    print(f'Usable Data: {len(file_names)}/{len(file_names_orig)}')\n",
    "    \n",
    "    for i in [ int(len(file_names)*p) for p in percentages ]:\n",
    "        training = file_names[:i]\n",
    "        test = file_names[i:] # file_names[i:] - other mode\n",
    "        print(f\"training: {len(training)}, test: {len(test)}\",end=\"\\r\", flush=True)\n",
    "        avg_waste, avg_retries, avg_runtime = run_simulation(directory, training, test, k = 4)\n",
    "        x.append(i)\n",
    "        y_waste.append(list(map(lambda w: round(w, 2),avg_waste)))\n",
    "        y_retries.append(avg_retries)\n",
    "        y_runtime.append(avg_runtime)\n",
    "\n",
    "    return (y_waste, y_retries, y_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ed02b-476f-4d1f-952b-ffde63fcc3cf",
   "metadata": {},
   "source": [
    "# Test folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16b40e3a-976b-477a-bee9-82ddefbc0141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable Data: 432/432\n"
     ]
    }
   ],
   "source": [
    "base_directory = f'{BASE_DIR}/sarek'\n",
    "workflow_tasks = [os.path.join(base_directory, item) for item in os.listdir(base_directory) if os.path.isdir(os.path.join(base_directory, item))]\n",
    "workflow_tasks = [task for task in workflow_tasks if len(os.listdir(task)) > 40]\n",
    "\n",
    "categories = [\"Wastage\", \"Retries\", \"Runtime\"]\n",
    "percentages = [\"25%\", \"50%\", \"75%\"]\n",
    "\n",
    "# 0 = WASTE, 1 = RETRIES, 2 = RUNTIME\n",
    "for task in workflow_tasks:\n",
    "    r = benchmark_task(task)\n",
    "    if r == -1:\n",
    "        continue\n",
    "    task_name = os.path.basename(task)\n",
    "    m = ', '.join(map(str, r[0][2]))\n",
    "    print(f'{task_name}')\n",
    "    for i, category in enumerate(categories): \n",
    "        for j, percentage in enumerate(percentages): \n",
    "            print(f'{category} {percentage}: {r[i][j]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65452b86-51a5-49ad-a6ec-fa1e90246504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
